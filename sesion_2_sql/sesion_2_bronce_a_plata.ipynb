{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62dce44d-3362-4015-a9d2-073d54b6799e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Cuaderno: 2. Procesamiento de Bronce a Plata**\n",
    "\n",
    "### **Carga de Datos desde la Capa Bronce**\n",
    "\n",
    "**Objetivo:** El primer paso en este cuaderno de transformación es cargar los datos que ya hemos ingerido. Esta celda se conecta a nuestra base de datos, lee las tablas de la capa Bronce y las carga en DataFrames de PySpark, preparándolas para el proceso de limpieza y refinamiento.\n",
    "\n",
    "-----\n",
    "\n",
    "## **Preparando el Entorno de Transformación**\n",
    "\n",
    "Antes de poder aplicar cualquier lógica de negocio, necesitamos tener acceso a los datos. En lugar de generar los datos nuevamente, nos conectamos directamente a las tablas Delta que creamos en el cuaderno anterior.\n",
    "\n",
    "Este proceso simula cómo un job o tarea de ETL (Extracción, Transformación y Carga) comenzaría su ejecución:\n",
    "\n",
    "1.  **Establecer el Contexto de la Base de Datos**: Indicamos a Spark que todas las operaciones subsecuentes deben realizarse dentro de nuestra base de datos `curso_arquitecturas`.\n",
    "2.  **Cargar las Tablas en DataFrames**: Leemos cada tabla de la capa Bronce y la asignamos a un DataFrame de PySpark. Nombrar las variables con un sufijo (ej. `_df`) es una buena práctica para identificar fácilmente que se trata de un DataFrame en memoria.\n",
    "3.  **Verificación Inicial**: Realizamos una acción simple como `.count()` para verificar que los datos se han cargado correctamente y para tener una idea del volumen de registros con el que vamos a trabajar.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e128b717-e141-4d12-addd-6f1db776937c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Celda 1: Configuración del entorno y carga de tablas de la capa Bronce.\n",
    "\n",
    "# Paso 1: Establecer la base de datos actual para la sesión de Spark.\n",
    "# Esto nos evita tener que escribir el nombre de la base de datos en cada consulta.\n",
    "db_name = \"curso_arquitecturas\"\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "print(f\"Contexto establecido en la base de datos: '{db_name}'\")\n",
    "\n",
    "# Paso 2: Leer cada tabla de la capa Bronce y cargarla en un DataFrame.\n",
    "try:\n",
    "    proveedores_bronze_df = spark.read.table(\"proveedores_bronze\")\n",
    "    usuarios_bronze_df = spark.read.table(\"usuarios_bronze\")\n",
    "    productos_bronze_df = spark.read.table(\"productos_bronze\")\n",
    "    pedidos_bronze_df = spark.read.table(\"pedidos_bronze\")\n",
    "    devoluciones_bronze_df = spark.read.table(\"devoluciones_bronze\")\n",
    "    \n",
    "    print(\"\\nCarga de datos desde la capa Bronce completada exitosamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar las tablas de la capa Bronce: {e}\")\n",
    "    # En un entorno de producción, aquí podrías detener la ejecución o enviar una alerta.\n",
    "    # dbutils.notebook.exit(\"Error crítico: No se pudieron cargar las tablas de origen.\")\n",
    "\n",
    "# Paso 3: Realizar una verificación rápida para confirmar que los DataFrames tienen datos.\n",
    "print(\"\\n--- Conteo de registros por tabla ---\")\n",
    "print(f\"Proveedores: {proveedores_bronze_df.count()} registros\")\n",
    "print(f\"Usuarios: {usuarios_bronze_df.count()} registros\")\n",
    "print(f\"Productos: {productos_bronze_df.count()} registros\")\n",
    "print(f\"Pedidos: {pedidos_bronze_df.count()} registros\")\n",
    "print(f\"Devoluciones: {devoluciones_bronze_df.count()} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19f8713-0f3b-4b7c-a2f4-9815bc97daf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pasar de la capa **Bronce** a la **Plata** es el proceso de transformar datos crudos y poco fiables en un conjunto de datos limpio, estructurado y listo para el análisis.\n",
    "\n",
    "Es el paso donde se aplica la calidad y se asegura que la información sea confiable.\n",
    "\n",
    "---\n",
    "## **El Proceso de Refinamiento**\n",
    "\n",
    "El objetivo es convertir datos en su estado original (Bronce) a un formato validado y útil (Plata) mediante estas acciones clave:\n",
    "\n",
    "* **Limpieza:** Se eliminan registros duplicados, se corrigen errores evidentes y se manejan los valores nulos.\n",
    "* **Validación:** Se asegura que cada columna tenga el tipo de dato correcto (ej. convertir texto a fecha o número) y que los identificadores importantes no estén vacíos.\n",
    "* **Estructuración:** Se aplican nombres de columna claros y consistentes y se organizan los datos en un modelo lógico y fácil de consultar.\n",
    "\n",
    "En resumen, la capa Plata toma el desorden de la capa Bronce y lo convierte en la **fuente única de la verdad** para la empresa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344081d2-99f7-4e0d-84e9-3b79c1096486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Justificación y Plan para el Modelo Dimensional (Kimball)**\n",
    "\n",
    "**Objetivo:** Antes de escribir el código de transformación, esta celda establece la estrategia que seguiremos. Adoptaremos formalmente la metodología de **modelado dimensional de Kimball** para estructurar nuestras tablas en la capa Plata.\n",
    "\n",
    "---\n",
    "## **¿Por Qué un Modelo Dimensional?**\n",
    "\n",
    "La estructura de los datos en la capa Plata no es arbitraria. Debe diseñarse específicamente para optimizar las consultas analíticas y facilitar la comprensión del negocio. El modelo Kimball es el estándar de la industria para este propósito por tres razones principales:\n",
    "\n",
    "1.  **Rendimiento en Consultas**: A diferencia de los modelos normalizados para sistemas transaccionales (que optimizan la escritura), el modelo dimensional (esquema en estrella) está diseñado para acelerar las operaciones de lectura y agregación, que son el 99% de las cargas de trabajo en un Data Warehouse.\n",
    "\n",
    "2.  **Claridad para el Análisis**: Separa los datos de forma intuitiva en **\"Hechos\"** (los números y métricas que queremos medir, como las ventas) y **\"Dimensiones\"** (el contexto que describe esos hechos: quién, qué, dónde, cuándo). Esta estructura es fácil de entender para los analistas y herramientas de BI.\n",
    "\n",
    "3.  **Estándar de la Industria**: Es una metodología probada, robusta y ampliamente adoptada, lo que facilita la mantenibilidad del proyecto y la incorporación de nuevos miembros al equipo.\n",
    "\n",
    "---\n",
    "## **Nuestro Plan de Acción**\n",
    "\n",
    "Para implementar el modelo, clasificaremos nuestras tablas `_silver` y crearemos una dimensión adicional clave:\n",
    "\n",
    "* **Identificar Tablas de Dimensión**: Estas tablas describen nuestras entidades de negocio. Serán:\n",
    "    * `usuarios_silver` (Dimensión de Cliente)\n",
    "    * `productos_silver` (Dimensión de Producto)\n",
    "    * `proveedores_silver` (Dimensión de Proveedor)\n",
    "\n",
    "* **Identificar Tablas de Hechos**: Estas tablas registran los eventos de negocio. Serán:\n",
    "    * `pedidos_silver` (Hechos de Ventas)\n",
    "    * `devoluciones_silver` (Hechos de Devoluciones)\n",
    "\n",
    "* **Crear una Dimensión de Tiempo (Mejor Práctica)**: Una de las prácticas fundamentales del modelo Kimball es no usar directamente las columnas de fecha (`TIMESTAMP`) de las tablas de hechos para el análisis. En su lugar, crearemos una **Dimensión de Fecha** (`dim_fecha`). Esta tabla contendrá una fila por cada día y columnas precalculadas (año, mes, nombre del mes, trimestre, día de la semana, etc.), lo que simplificará y acelerará enormemente las consultas basadas en tiempo.\n",
    "\n",
    "Al finalizar las siguientes celdas, nuestra capa Plata estará organizada como un esquema en estrella, listo para servir análisis o para construir las agregaciones finales en la capa Oro.\n",
    "\n",
    "![ERD](ERD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47198eae-2471-498f-9b86-7e00ad38950c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Intermezzo\n",
    "\n",
    "El flujo de diseño siempre sigue esta secuencia: **Conceptual -> Lógico -> Físico**.\n",
    "\n",
    "---\n",
    "## 1. Modelo Conceptual\n",
    "\n",
    "**¿Qué es?** Es la vista de más alto nivel. Identifica las principales **entidades de negocio** y las relaciones entre ellas, sin entrar en detalles de atributos o llaves. Responde a la pregunta: ¿de qué se trata este negocio?\n",
    "\n",
    "**¿Cuándo lo hicimos en el taller?**\n",
    "Lo hicimos al principio, en la **primera celda**, cuando decidimos qué datos necesitábamos generar para que nuestro escenario fuera realista. En ese momento, identificamos los conceptos clave:\n",
    "* Hay **Usuarios** que realizan **Pedidos**.\n",
    "* Los **Pedidos** contienen **Productos**.\n",
    "* Los **Productos** son suministrados por **Proveedores**.\n",
    "* A veces, los **Pedidos** resultan en **Devoluciones**.\n",
    "\n",
    "Esa discusión inicial y la planificación de las entidades a generar fue nuestro modelo conceptual.\n",
    "\n",
    "\n",
    "---\n",
    "## 2. Modelo Lógico\n",
    "\n",
    "**¿Qué es?** Es el plano detallado de nuestra base de datos, pero sin atarse a una tecnología específica. Aquí definimos las tablas, las columnas (atributos), los tipos de datos generales (número, texto, fecha), y las **claves primarias y foráneas** que conectan las tablas.\n",
    "\n",
    "**¿Cuándo lo hicimos en el taller?**\n",
    "Lo construimos en la fase de justificación, justo antes de empezar a escribir el código para la capa Plata:\n",
    "* Cuando decidimos usar la **metodología Kimball** y su **esquema en estrella**.\n",
    "* Cuando diseñamos el **Diagrama Entidad-Relación (ERD) en Mermaid**. Ese diagrama es la representación visual de nuestro modelo lógico. Define con precisión cada tabla, columna y relación.\n",
    "\n",
    "*es decir aqui arribita*\n",
    "\n",
    "---\n",
    "## 3. Modelo Físico\n",
    "\n",
    "**¿Qué es?** Es la implementación real y tecnológica del modelo lógico en un sistema de base de datos específico. Aquí es donde se toman decisiones sobre el motor de almacenamiento, los tipos de datos exactos, los índices y las optimizaciones físicas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd0e017-428a-4ec2-a575-f173b210462b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----\n",
    "Ya que conocemos el ERD y lo que buscamos construir, vamos a crear las tablas en la capa Plata con las llaves apropiadas. Para ello, seguiremos un proceso de tres pasos:\n",
    "\n",
    "1.  Crear las Tablas de Dimensión y declarar sus claves primarias.\n",
    "2.  Construir la Dimensión de Fecha, una pieza clave en nuestro modelo.\n",
    "3.  Crear las Tablas de Hechos y, lo más importante, establecer formalmente las relaciones con las dimensiones mediante claves foráneas.\n",
    "\n",
    "Comencemos.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Paso 1: Creación de las Dimensiones y Declaración de sus Claves Primarias**\n",
    "\n",
    "**Objetivo:** Crear las tablas de dimensión y, acto seguido, declarar sus claves primarias (PK).\n",
    "\n",
    "**Código:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24118e09-3aeb-405a-8189-cdfa1e4603a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE curso_arquitecturas;\n",
    "\n",
    "-- **Dimensión de Usuarios**\n",
    "-- Paso 1: Crear la tabla con el esquema definido, especificando NOT NULL en la PK.\n",
    "CREATE OR REPLACE TABLE usuarios_silver (\n",
    "  id_usuario BIGINT NOT NULL,\n",
    "  nombre_usuario STRING,\n",
    "  email STRING,\n",
    "  fecha_registro TIMESTAMP,\n",
    "  ciudad STRING\n",
    ") COMMENT 'Dimensión de clientes con PK definida como NOT NULL.';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee49dd90-cfa1-426b-b9b0-fb86375bcbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM curso_arquitecturas.usuarios_silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4729a1e9-a64d-400d-afe9-0585ea6355e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 2: Insertar los datos en la tabla ya creada.\n",
    "INSERT INTO usuarios_silver\n",
    "SELECT CAST(id_usuario AS BIGINT),\n",
    "nombre,\n",
    "email,\n",
    "CAST(fecha_registro AS TIMESTAMP),\n",
    "ciudad\n",
    "FROM usuarios_bronze\n",
    "WHERE id_usuario IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c918925-a181-43fb-90d2-297076c4f63b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 3: Ahora la declaración de la PK funcionará.\n",
    "ALTER TABLE usuarios_silver ADD CONSTRAINT pk_usuarios PRIMARY KEY(id_usuario) NOT ENFORCED;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1023bf32-2434-4ed9-ac9d-0ffe4bb370d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Dimensión de Proveedores**\n",
    "CREATE OR REPLACE TABLE proveedores_silver (\n",
    "  id_proveedor BIGINT NOT NULL,\n",
    "  nombre_proveedor STRING\n",
    ");\n",
    "INSERT INTO proveedores_silver\n",
    "SELECT CAST(id_proveedor AS BIGINT), nombre_proveedor\n",
    "FROM proveedores_bronze WHERE id_proveedor IS NOT NULL;\n",
    "ALTER TABLE proveedores_silver ADD CONSTRAINT pk_proveedores PRIMARY KEY(id_proveedor) NOT ENFORCED;\n",
    "\n",
    "\n",
    "-- **Dimensión de Productos**\n",
    "CREATE OR REPLACE TABLE productos_silver (\n",
    "  id_producto BIGINT NOT NULL,\n",
    "  id_proveedor BIGINT,\n",
    "  nombre_producto STRING,\n",
    "  categoria STRING,\n",
    "  precio_unitario DECIMAL(10, 2)\n",
    ");\n",
    "INSERT INTO productos_silver\n",
    "SELECT CAST(id_producto AS BIGINT), CAST(id_proveedor AS BIGINT), nombre_producto, categoria, CAST(precio_unitario AS DECIMAL(10, 2))\n",
    "FROM productos_bronze WHERE id_producto IS NOT NULL;\n",
    "ALTER TABLE productos_silver ADD CONSTRAINT pk_productos PRIMARY KEY(id_producto) NOT ENFORCED;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577ceedd-e644-4faf-9f8c-fa9c82f1f420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **¿Por qué declaramos estas Claves Primarias?**\n",
    "\n",
    "Aunque son `NOT ENFORCED` (no forzadas), al declarar una Clave Primaria le estamos dando al optimizador de consultas una garantía: **los valores en esta columna son únicos**. Con esta información, el motor puede simplificar los planes de consulta, por ejemplo, al evitar pasos de eliminación de duplicados después de un `JOIN`, sabiendo de antemano que la relación es de uno a muchos.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750c54b7-7370-43e5-9fcb-852ce2c03307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Paso 2: Construcción de la Dimensión de Fecha**\n",
    "\n",
    "**Objetivo:** Construir la `dim_fecha` y declarar su clave primaria, un requisito para poder vincularla de manera eficiente a las tablas de hechos.\n",
    "\n",
    "**Código:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc349399-536f-4ea3-b5f7-f32ba750397a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT SEQUENCE(TO_DATE('2022-01-01'), TO_DATE('2026-12-31'), INTERVAL 1 DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70fb9916-7e76-427b-9a72-27616a620a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT EXPLODE(SEQUENCE(TO_DATE('2022-01-01'), TO_DATE('2026-12-31'), INTERVAL 1 DAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08f7460-32be-4bfd-bd59-e928d613ecfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 1: Crear la tabla vacía con el esquema definido, especificando NOT NULL en la PK.\n",
    "CREATE OR REPLACE TABLE dim_fecha (\n",
    "  id_fecha DATE NOT NULL,\n",
    "  anio INT,\n",
    "  mes INT,\n",
    "  dia INT,\n",
    "  trimestre INT,\n",
    "  nombre_mes STRING,\n",
    "  nombre_dia_semana STRING,\n",
    "  tipo_dia STRING\n",
    ") COMMENT 'Dimensión de fecha con PK definida como NOT NULL.';\n",
    "\n",
    "-- Paso 2: Insertar los datos en la tabla ya creada.\n",
    "INSERT INTO dim_fecha\n",
    "SELECT\n",
    "  fecha AS id_fecha,\n",
    "  YEAR(fecha) AS anio,\n",
    "  MONTH(fecha) AS mes,\n",
    "  DAY(fecha) AS dia,\n",
    "  QUARTER(fecha) AS trimestre,\n",
    "  DATE_FORMAT(fecha, 'MMMM') AS nombre_mes, \n",
    "  DATE_FORMAT(fecha, 'EEEE') AS nombre_dia_semana,\n",
    "  CASE WHEN DAYOFWEEK(fecha) IN (1, 7) THEN 'Fin de Semana' ELSE 'Día de Semana' END AS tipo_dia\n",
    "FROM (\n",
    "  SELECT EXPLODE(SEQUENCE(TO_DATE('2022-01-01'), TO_DATE('2026-12-31'), INTERVAL 1 DAY)) AS fecha\n",
    ");\n",
    "\n",
    "-- Paso 3: Ahora la declaración de la PK funcionará.\n",
    "ALTER TABLE dim_fecha ADD CONSTRAINT pk_dim_fecha PRIMARY KEY(id_fecha) NOT ENFORCED;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd38660d-6981-4775-9133-b0c98e91e76c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----\n",
    "\n",
    "### **Paso 3: Creación de Tablas de Hechos y sus Claves Foráneas**\n",
    "\n",
    "**Objetivo:** Construir las tablas de hechos y declarar las relaciones con sus dimensiones a través de las claves foráneas (FK). Este es el paso final para completar nuestro esquema en estrella.\n",
    "\n",
    "**Código:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3634aa4-c5f8-4686-9f6e-6e88ea71895a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Tabla de Hechos de Pedidos**\n",
    "-- Paso 1: Crear la tabla vacía con el esquema definido, especificando NOT NULL en la PK.\n",
    "CREATE OR REPLACE TABLE pedidos_silver (\n",
    "  id_pedido BIGINT NOT NULL,\n",
    "  id_usuario BIGINT,\n",
    "  id_producto BIGINT,\n",
    "  id_fecha DATE,\n",
    "  cantidad INT,\n",
    "  monto_total DECIMAL(18, 2),\n",
    "  ts_pedido TIMESTAMP\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae05da1-9681-4785-9f12-dcd307ee2402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 2: Insertar los datos en la tabla ya creada.\n",
    "INSERT INTO pedidos_silver\n",
    "SELECT \n",
    "  p.id_pedido, \n",
    "  CAST(p.id_usuario AS BIGINT), \n",
    "  CAST(p.id_producto AS BIGINT), \n",
    "  TO_DATE(p.fecha_pedido), \n",
    "  CAST(p.cantidad AS INT), \n",
    "  CAST(p.monto AS DECIMAL(18, 2)), \n",
    "  CAST(p.fecha_pedido AS TIMESTAMP)\n",
    "FROM pedidos_bronze AS p \n",
    "WHERE p.id_pedido IS NOT NULL AND p.id_usuario IS NOT NULL AND p.id_producto IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18880b20-7033-4ead-a395-6b7fda1cc846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 3: Ahora las declaraciones de llaves funcionarán.\n",
    "ALTER TABLE pedidos_silver ADD CONSTRAINT pk_pedidos PRIMARY KEY(id_pedido) NOT ENFORCED;\n",
    "ALTER TABLE pedidos_silver ADD CONSTRAINT fk_pedidos_usuarios FOREIGN KEY(id_usuario) REFERENCES usuarios_silver(id_usuario) NOT ENFORCED;\n",
    "ALTER TABLE pedidos_silver ADD CONSTRAINT fk_pedidos_productos FOREIGN KEY(id_producto) REFERENCES productos_silver(id_producto) NOT ENFORCED;\n",
    "ALTER TABLE pedidos_silver ADD CONSTRAINT fk_pedidos_fecha FOREIGN KEY(id_fecha) REFERENCES dim_fecha(id_fecha) NOT ENFORCED;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1572d3fe-70b4-4560-8d7a-6898016a7595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Tabla de Hechos de Devoluciones**\n",
    "CREATE OR REPLACE TABLE devoluciones_silver (\n",
    "  id_devolucion BIGINT NOT NULL,\n",
    "  id_pedido BIGINT,\n",
    "  id_fecha DATE,\n",
    "  motivo STRING,\n",
    "  ts_devolucion TIMESTAMP\n",
    ");\n",
    "INSERT INTO devoluciones_silver\n",
    "SELECT \n",
    "  CAST(d.id_devolucion AS BIGINT), \n",
    "  CAST(d.id_pedido AS BIGINT), \n",
    "  TO_DATE(d.fecha_devolucion), \n",
    "  d.motivo, \n",
    "  CAST(d.fecha_devolucion AS TIMESTAMP)\n",
    "FROM devoluciones_bronze AS d \n",
    "WHERE d.id_devolucion IS NOT NULL AND d.id_pedido IS NOT NULL;\n",
    "\n",
    "-- Declaración de Restricciones\n",
    "ALTER TABLE devoluciones_silver ADD CONSTRAINT pk_devoluciones PRIMARY KEY(id_devolucion) NOT ENFORCED;\n",
    "ALTER TABLE devoluciones_silver ADD CONSTRAINT fk_devoluciones_pedidos FOREIGN KEY(id_pedido) REFERENCES pedidos_silver(id_pedido) NOT ENFORCED;\n",
    "ALTER TABLE devoluciones_silver ADD CONSTRAINT fk_devoluciones_fecha FOREIGN KEY(id_fecha) REFERENCES dim_fecha(id_fecha) NOT ENFORCED;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2de918-0eda-4100-9837-46ea5731b120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### **¿Por qué declaramos estas Claves Foráneas?**\n",
    "\n",
    "Declarar las claves foráneas es el paso de optimización más crítico. Al hacerlo, le damos al optimizador un \"mapa\" de cómo se conectan las tablas. Esto permite dos optimizaciones clave:\n",
    "\n",
    "1.  **Reordenamiento de `JOINs`**: El optimizador puede elegir el orden más eficiente para unir las tablas, empezando por las uniones que más reduzcan los datos a procesar.\n",
    "2.  **Eliminación de `JOINs`**: Si una consulta une `pedidos_silver` con `usuarios_silver` solo para filtrar por `id_usuario`, el optimizador, al ver la relación FK-PK, entiende que el `JOIN` es innecesario y lo elimina, leyendo únicamente la tabla de hechos. Esto acelera drásticamente la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50fed0c-a9ef-461b-95ec-4f78ce253719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  categoria,\n",
    "  sum(monto_total),\n",
    "  mes,\n",
    "  anio\n",
    "from\n",
    "  pedidos_silver\n",
    "    INNER JOIN productos_silver\n",
    "      ON pedidos_silver.id_producto = productos_silver.id_producto\n",
    "    inner JOIN dim_fecha\n",
    "      ON pedidos_silver.id_fecha = dim_fecha.id_fecha\n",
    "GROUP BY\n",
    "  categoria,\n",
    "  mes,\n",
    "  anio\n",
    "ORDER BY\n",
    "  anio,\n",
    "  mes\"\"\").show()\n",
    "\n",
    "end_time= time.time()\n",
    "\n",
    "elapsed_time= end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {elapsed_time:.3f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76959248-7c84-4d6a-8b5d-e9b9a74708da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### **Implementación del Modelo Físico Optimizado**\n",
    "\n",
    "Ahora vamos a transformar nuestro modelo dimensional, que ya es estructuralmente correcto, en un modelo de alto rendimiento. Para ello, aplicaremos técnicas de optimización física que organizan la disposición de los datos, permitiendo a Databricks leer la menor cantidad de información posible para resolver una consulta.\n",
    "\n",
    "#### **La Estrategia: Habilitar el \"Data Skipping\"**\n",
    "\n",
    "Un modelo sin optimización física obliga a Databricks a realizar un escaneo completo de todos los archivos de datos para la mayoría de las consultas, lo cual es ineficiente y costoso a gran escala.\n",
    "\n",
    "Para evitarlo, aplicaremos las dos técnicas de optimización física más importantes en Delta Lake para habilitar el **\"data skipping\"** (omisión de datos):\n",
    "\n",
    "1.  **Particionamiento (`PARTITIONED BY`)**: Esta técnica divide una tabla en subdirectorios basados en los valores de una columna. Cuando una consulta filtra por esa columna, Databricks solo lee los directorios relevantes, ignorando el resto. Es la optimización más efectiva para columnas de **baja cardinalidad** (pocos valores distintos), como fechas, países o categorías.\n",
    "\n",
    "2.  **Z-Ordering (`ZORDER BY`)**: Esta es una técnica más fina que reorganiza los datos *dentro* de los archivos. Agrupa valores de columnas relacionadas para que estén físicamente cerca. Esto mejora drásticamente la capacidad del motor para \"saltarse\" bloques de datos dentro de un archivo, incluso sin un filtro de partición. Es ideal para columnas de **alta cardinalidad** que se usan frecuentemente en filtros o `JOINs`, como las claves primarias y foráneas (`id_usuario`, `id_producto`).\n",
    "\n",
    "A continuación, reconstruiremos nuestras tablas de la capa Plata aplicando estas técnicas.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Creación de Dimensiones Optimizadas**\n",
    "\n",
    "**Objetivo:** Crear las tablas de dimensión, aplicando particionamiento y Z-Ordering para acelerar las futuras consultas y uniones.\n",
    "\n",
    "**Código:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e453ce2-d8f7-4949-a194-71f238692b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE curso_arquitecturas;\n",
    "\n",
    "-- **Dimensión de Usuarios (Optimizada con Z-Ordering)**\n",
    "CREATE OR REPLACE TABLE usuarios_silver (\n",
    "  id_usuario BIGINT NOT NULL, \n",
    "  nombre_usuario STRING,\n",
    "  email STRING,\n",
    "  fecha_registro TIMESTAMP, ciudad STRING\n",
    ") COMMENT 'Dimensión de clientes optimizada con Z-Ordering en su PK.';\n",
    "\n",
    "\n",
    "INSERT INTO usuarios_silver\n",
    "SELECT \n",
    "CAST(id_usuario AS BIGINT),\n",
    "nombre,\n",
    "email,\n",
    "CAST(fecha_registro AS TIMESTAMP),\n",
    "ciudad\n",
    "FROM usuarios_bronze WHERE id_usuario IS NOT NULL;\n",
    "\n",
    "---------- ESTE ES EL PASO OPTIMIZADOR ----------------\n",
    "OPTIMIZE usuarios_silver ZORDER BY (id_usuario);\n",
    "---------- ESTE ES EL PASO OPTIMIZADOR ----------------\n",
    "\n",
    "ALTER TABLE usuarios_silver ADD CONSTRAINT pk_usuarios PRIMARY KEY(id_usuario) NOT ENFORCED;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa1e1e7-a1d7-4d03-a8f1-5972060ba4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Dimensión de Productos (Optimizada con Particionamiento y Z-Ordering)**\n",
    "CREATE OR REPLACE TABLE productos_silver (\n",
    "  id_producto BIGINT NOT NULL,\n",
    "  id_proveedor BIGINT,\n",
    "  nombre_producto STRING,\n",
    "  precio_unitario DECIMAL(10, 2),\n",
    "  categoria STRING\n",
    ")\n",
    "----- PASO OPTIMIZADOR\n",
    "PARTITIONED BY (categoria)\n",
    "--------\n",
    "COMMENT 'Dimensión de productos particionada por categoría y optimizada por su PK.';\n",
    "\n",
    "INSERT INTO productos_silver\n",
    "SELECT \n",
    "CAST(id_producto AS BIGINT), \n",
    "CAST(id_proveedor AS BIGINT), \n",
    "nombre_producto, \n",
    "CAST(precio_unitario AS DECIMAL(10, 2)),\n",
    "categoria\n",
    "FROM productos_bronze WHERE id_producto IS NOT NULL;\n",
    "\n",
    "----- PASO OPTIMIZADOR\n",
    "OPTIMIZE productos_silver ZORDER BY (id_producto);\n",
    "--------\n",
    "\n",
    "ALTER TABLE productos_silver ADD CONSTRAINT pk_productos PRIMARY KEY(id_producto) NOT ENFORCED;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c786c1-351c-444d-8ed8-231ad9c4247e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Explicación de la Optimización:** Se particiona `productos_silver` por `categoria` porque es una columna de baja cardinalidad, lo que acelerará los filtros por este campo. Se aplica `ZORDER BY` en las claves primarias (`id_usuario`, `id_producto`) porque son columnas de alta cardinalidad usadas en los `JOINs`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1304196-ed93-4024-b836-2ee1b885515e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creación de la Dimensión de Fecha**\n",
    "\n",
    "*(Esta tabla generalmente no requiere optimizaciones adicionales debido a su naturaleza secuencial, por lo que el código no cambia)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb15746-cc10-48ad-b8cf-61fa8ba9bf29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE dim_fecha (\n",
    "  id_fecha DATE NOT NULL, \n",
    "  anio INT, \n",
    "  mes INT, \n",
    "  dia INT, \n",
    "  trimestre INT, \n",
    "  nombre_mes STRING, \n",
    "  nombre_dia_semana STRING, \n",
    "  tipo_dia STRING);\n",
    "\n",
    "INSERT INTO dim_fecha \n",
    "SELECT fecha, \n",
    "  YEAR(fecha), \n",
    "  MONTH(fecha),\n",
    "  DAY(fecha), \n",
    "  QUARTER(fecha), \n",
    "  DATE_FORMAT(fecha, 'MMMM'), \n",
    "  DATE_FORMAT(fecha, 'EEEE'), CASE WHEN DAYOFWEEK(fecha) IN (1, 7) THEN 'Fin de Semana' ELSE 'Día de Semana' END \n",
    "FROM (SELECT EXPLODE(SEQUENCE(TO_DATE('2022-01-01'), TO_DATE('2026-12-31'), INTERVAL 1 DAY)) AS fecha);\n",
    "\n",
    "\n",
    "\n",
    "ALTER TABLE dim_fecha ADD CONSTRAINT pk_dim_fecha PRIMARY KEY(id_fecha) NOT ENFORCED;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1adbc305-c2a5-4723-b853-0adc97110ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Celda 3: Creación de Tablas de Hechos Optimizadas**\n",
    "\n",
    "**Objetivo:** Aplicar la estrategia de optimización más importante a nuestras tablas de hechos, particionando por fecha y aplicando Z-Ordering en las claves foráneas.\n",
    "\n",
    "**Código:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02078339-73a9-4054-9af5-b91da243d091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Tabla de Hechos de Pedidos**\n",
    "CREATE OR REPLACE TABLE pedidos_silver(\n",
    "  id_pedido BIGINT NOT NULL,\n",
    "  id_usuario BIGINT,\n",
    "  id_producto BIGINT,\n",
    "  cantidad INT,\n",
    "  monto_total DECIMAL(18, 2),\n",
    "  ts_pedido TIMESTAMP,\n",
    "  id_fecha DATE\n",
    ")\n",
    "COMMENT 'Tabla de hechos de ventas, particionada por fecha para optimizar consultas de series de tiempo.';\n",
    "\n",
    "INSERT INTO pedidos_silver\n",
    "  SELECT\n",
    "    CAST(p.id_pedido AS BIGINT),\n",
    "    CAST(p.id_usuario AS BIGINT),\n",
    "    CAST(p.id_producto AS BIGINT),\n",
    "    CAST(p.cantidad AS INT),\n",
    "    CAST(p.monto AS DECIMAL(18, 2)),\n",
    "    CAST(p.fecha_pedido AS TIMESTAMP),\n",
    "    TO_DATE(p.fecha_pedido)\n",
    "  FROM\n",
    "    pedidos_bronze AS p\n",
    "  WHERE\n",
    "    p.id_pedido IS NOT NULL\n",
    "    AND p.id_usuario IS NOT NULL\n",
    "    AND p.id_producto IS NOT NULL;\n",
    "\n",
    "OPTIMIZE\n",
    "  pedidos_silver\n",
    "ZORDER BY (id_fecha, id_usuario, id_producto);\n",
    "\n",
    "ALTER TABLE\n",
    "  pedidos_silver\n",
    "ADD\n",
    "  CONSTRAINT pk_pedidos PRIMARY KEY (id_pedido) NOT ENFORCED;\n",
    "\n",
    "ALTER TABLE\n",
    "  pedidos_silver\n",
    "ADD\n",
    "  CONSTRAINT fk_pedidos_usuarios\n",
    "    FOREIGN KEY (id_usuario) REFERENCES usuarios_silver (id_usuario) NOT ENFORCED;\n",
    "\n",
    "ALTER TABLE\n",
    "  pedidos_silver\n",
    "ADD\n",
    "  CONSTRAINT fk_pedidos_productos\n",
    "    FOREIGN KEY (id_producto) REFERENCES productos_silver (id_producto) NOT ENFORCED;\n",
    "\n",
    "ALTER TABLE\n",
    "  pedidos_silver\n",
    "ADD\n",
    "  CONSTRAINT fk_pedidos_fecha FOREIGN KEY (id_fecha) REFERENCES dim_fecha (id_fecha) NOT ENFORCED;\n",
    "\n",
    "-- **Tabla de Hechos de Devoluciones**\n",
    "CREATE OR REPLACE TABLE devoluciones_silver(\n",
    "  id_devolucion BIGINT NOT NULL,\n",
    "  id_pedido BIGINT,\n",
    "  motivo STRING,\n",
    "  ts_devolucion TIMESTAMP,\n",
    "  id_fecha DATE\n",
    ")\n",
    "PARTITIONED BY (id_fecha);\n",
    "\n",
    "INSERT INTO devoluciones_silver\n",
    "  SELECT\n",
    "    CAST(d.id_devolucion AS BIGINT),\n",
    "    CAST(d.id_pedido AS BIGINT),\n",
    "    d.motivo,\n",
    "    CAST(d.fecha_devolucion AS TIMESTAMP),\n",
    "    TO_DATE(d.fecha_devolucion)\n",
    "  FROM\n",
    "    devoluciones_bronze AS d\n",
    "  WHERE\n",
    "    d.id_devolucion IS NOT NULL\n",
    "    AND d.id_pedido IS NOT NULL;\n",
    "\n",
    "OPTIMIZE\n",
    "  devoluciones_silver\n",
    "ZORDER BY (id_pedido);\n",
    "\n",
    "ALTER TABLE\n",
    "  devoluciones_silver\n",
    "ADD\n",
    "  CONSTRAINT pk_devoluciones PRIMARY KEY (id_devolucion) NOT ENFORCED;\n",
    "\n",
    "ALTER TABLE\n",
    "  devoluciones_silver\n",
    "ADD\n",
    "  CONSTRAINT fk_devoluciones_pedidos\n",
    "    FOREIGN KEY (id_pedido) REFERENCES pedidos_silver (id_pedido) NOT ENFORCED;\n",
    "\n",
    "ALTER TABLE\n",
    "  devoluciones_silver\n",
    "ADD\n",
    "  CONSTRAINT fk_devoluciones_fecha\n",
    "    FOREIGN KEY (id_fecha) REFERENCES dim_fecha (id_fecha) NOT ENFORCED;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "636c18bb-f3f7-49d0-8c83-0570385f5953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  * **Explicación de la Optimización:** Particionar las tablas de hechos por `id_fecha` es la estrategia más efectiva, ya que la mayoría de las consultas analíticas filtran por rangos de tiempo. `ZORDER BY` en las claves foráneas (`id_usuario`, `id_producto`) co-localiza los datos relacionados, acelerando significativamente los `JOINs` con las dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c41ffe5-8ede-4ea1-83c1-7008b5b630c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  categoria,\n",
    "  sum(monto_total),\n",
    "  mes,\n",
    "  anio\n",
    "from\n",
    "  pedidos_silver\n",
    "    INNER JOIN productos_silver\n",
    "      ON pedidos_silver.id_producto = productos_silver.id_producto\n",
    "    inner JOIN dim_fecha\n",
    "      ON pedidos_silver.id_fecha = dim_fecha.id_fecha\n",
    "GROUP BY\n",
    "  categoria,\n",
    "  mes,\n",
    "  anio\n",
    "ORDER BY\n",
    "  anio,\n",
    "  mes\"\"\").show()\n",
    "\n",
    "end_time= time.time()\n",
    "\n",
    "elapsed_time= end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {elapsed_time:.3f} segundos\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7502565160547211,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sesion_2_bronce_a_plata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
