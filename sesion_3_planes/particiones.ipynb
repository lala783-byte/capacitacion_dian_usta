{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ccec68-0916-432a-bf8f-0dba93c68364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sesión 3 - Cuaderno 2: Evidencia Práctica del \"Data Skipping\"\n",
    "\n",
    "### **Introducción: De la Teoría a la Práctica de la Optimización**\n",
    "\n",
    "En la Sesión 2, tomamos una decisión de diseño crucial: particionamos nuestras tablas de hechos (`pedidos_silver` y `devoluciones_silver`) por la columna `id_fecha`. La teoría nos dice que esto debería acelerar drásticamente cualquier consulta que filtre por un rango de fechas, ya que permite a Spark ignorar (o \"saltarse\") los datos que no son relevantes.\n",
    "\n",
    "Hoy, vamos a dejar de confiar en la teoría y pasaremos a la **verificación empírica**. Usaremos el comando `EXPLAIN` que aprendimos en el cuaderno anterior para analizar el plan de ejecución y encontrar la prueba definitiva de que nuestra estrategia de particionamiento está funcionando como se esperaba.\n",
    "\n",
    "**Objetivo:** Al finalizar este cuaderno, serás capaz de:\n",
    "\n",
    "* Identificar en un plan de ejecución cuándo Spark está realizando un escaneo completo de tabla (`Full Scan`).\n",
    "* Localizar y entender la sección `PartitionFilters` como la evidencia del \"data skipping\".\n",
    "* Cuantificar el impacto del particionamiento en el rendimiento de una consulta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de108bd0-af96-463a-a1e7-0d330edf3519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Paso 1: Preparación del Entorno**\n",
    "\n",
    "Como siempre, el primer paso es establecer el contexto de nuestra base de datos para asegurarnos de que estamos trabajando con las tablas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd02c121-3b6f-45ff-b58e-03742412b81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE curso_arquitecturas;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1760fa61-c4b5-489b-97e3-094771cae222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Paso 2: El Escenario Base - Consulta Sin Filtro de Partición**\n",
    "\n",
    "Para apreciar el beneficio del particionamiento, primero debemos establecer una línea base. Analizaremos el plan de una consulta que filtra la tabla `pedidos_silver` por una columna que **no es** la columna de partición (en este caso, `cantidad`).\n",
    "\n",
    "Teóricamente, esperamos que Spark se vea forzado a leer todos los datos de la tabla porque no tiene forma de saber en qué particiones (carpetas) se encuentran los pedidos con una `cantidad` mayor a 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab787ce-da4e-42e9-8a03-10f487e23836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "EXPLAIN SELECT *\n",
    "FROM pedidos_silver\n",
    "WHERE cantidad > 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd358be5-758e-43c3-8884-c1960577e028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*)\n",
    "FROM pedidos_silver\n",
    "WHERE cantidad > 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0511d079-c1fd-4e36-9237-c5e3c71133a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM pedidos_silver\n",
    "WHERE cantidad > 4\n",
    "LIMIT 5000000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "737294f1-6d4d-4f69-8124-1626498f518b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Paso 3: Análisis del Plan - El Costoso \"Full Scan\"**\n",
    "\n",
    "El resultado del plan físico se verá similar a esto:\n",
    "```\n",
    "== Physical Plan ==\n",
    "*(1) ColumnarToRow\n",
    "+- PhotonResultStage\n",
    "+- PhotonScan parquet workspace.curso_arquitecturas.pedidos_silver [...] DataFilters: [isnotnull(cantidad#...), (cantidad#... > 4)], PartitionFilters: [], ...\n",
    "```\n",
    "Observa con atención el `PhotonScan`. Hay dos puntos clave en este plan:\n",
    "\n",
    "1.  **`DataFilters: [..., (cantidad#... > 4)]`**: El optimizador ha aplicado correctamente el **Predicate Pushdown**. El filtro se aplica en el nivel más bajo posible.\n",
    "2.  **`PartitionFilters: []`**: ¡Esta es la parte crucial! La lista de `PartitionFilters` está **vacía**. Esto nos confirma que Spark **no pudo eliminar ninguna partición** y, por lo tanto, se vio obligado a escanear todos los subdirectorios y todos los archivos de la tabla `pedidos_silver` para encontrar las filas que cumplían la condición. Esto es un **escaneo completo de tabla (Full Table Scan)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4de9b15b-437a-4bf0-a4e2-1d640fd5e603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Paso 4: La Prueba Definitiva - Consulta Con Filtro de Partición**\n",
    "\n",
    "Ahora, realicemos el experimento contrario. Ejecutaremos una consulta muy similar, pero esta vez filtraremos por la columna de partición `id_fecha`.\n",
    "\n",
    "Según nuestra hipótesis, Spark debería ser lo suficientemente inteligente como para leer **únicamente** el subdirectorio correspondiente a la fecha '2023-01-15', ignorando todos los demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efd3240-57e5-49df-8335-fc242e130eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "EXPLAIN SELECT *\n",
    "FROM devoluciones_silver\n",
    "WHERE id_fecha = '2023-01-15';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7220dd4-e1b6-433c-a738-30b9092cfa06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM devoluciones_silver\n",
    "WHERE id_fecha = '2024-01-15';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34d6c1e-e1e8-46cd-b3b7-d39719c2a2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM devoluciones_silver\n",
    "WHERE id_fecha >= '2024-01-15' AND id_fecha <= '2024-06-15';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d3469c-03f2-41eb-a812-297cb2ec97f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM pedidos_silver\n",
    "WHERE id_fecha >= '2024-01-15' AND id_fecha <= '2024-06-15';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd51b848-c6af-49dd-9857-2c3a52d498d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Paso 5: Análisis del Plan - \"Data Skipping\" en Acción**\n",
    "\n",
    "El plan físico resultante será notablemente diferente:\n",
    "```\n",
    "== Physical Plan ==\n",
    "*(1) ColumnarToRow\n",
    "+- PhotonResultStage\n",
    "+- PhotonScan parquet workspace.curso_arquitecturas.pedidos_silver [...] PartitionFilters: [isnotnull(id_fecha#...), (id_fecha#... = 2023-01-15)] ...\n",
    "```\n",
    "¡Bingo! El plan físico ha cambiado drásticamente.\n",
    "\n",
    "* **`PartitionFilters: [..., (id_fecha#... = 2023-01-15)]`**: Esta es la prueba que estábamos buscando. La condición del `WHERE` ya no está en `DataFilters`, sino que ha sido promovida a `PartitionFilters`.\n",
    "\n",
    "Esto le indica a Spark que no necesita abrir ni un solo archivo Parquet que no esté dentro del directorio `/id_fecha=2023-01-15/`. Si nuestra tabla tiene datos de cientos de días diferentes, esta consulta ha ignorado el 99% de los datos, resultando en una reducción masiva del I/O y un aumento exponencial en la velocidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32c0ddf1-f0cf-49d3-83f0-2d63e92068cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Paso 6: Profundizando - Más Ejemplos de Filtros de Partición**\n",
    "\n",
    "La eliminación de particiones no solo funciona con el operador de igualdad (`=`). El optimizador de Spark es lo suficientemente avanzado como para aplicarla en escenarios más complejos.\n",
    "\n",
    "#### **Ejemplo 1: Filtrando con una lista de fechas (`IN`)**\n",
    "\n",
    "¿Qué pasa si queremos analizar datos de dos días específicos?\n",
    "\n",
    "**Análisis:** En el plan físico, verás que los `PartitionFilters` contendrán la condición `IN`. Spark es lo suficientemente inteligente como para escanear únicamente los dos directorios correspondientes a esas fechas, ignorando todo lo demás.\n",
    "\n",
    "#### **Ejemplo 2: Filtrando con un rango de fechas (`BETWEEN`)**\n",
    "\n",
    "Este es un caso de uso muy común en análisis de negocio, por ejemplo, para calcular las ventas de la última semana.\n",
    "\n",
    "**Análisis:** El plan mostrará de nuevo que la condición `BETWEEN` se ha traducido a `PartitionFilters`. Spark escaneará únicamente las 7 particiones correspondientes a la primera semana de enero de 2024, haciendo la agregación mucho más rápida.\n",
    "\n",
    "#### **Ejemplo 3: Filtrando con funciones de fecha**\n",
    "\n",
    "¿Puede Spark seguir optimizando si usamos funciones en la columna de partición? ¡Sí!\n",
    "\n",
    "**Análisis:** Aunque hemos aplicado funciones, Catalyst puede inferir que esta condición equivale a `id_fecha BETWEEN '2022-12-01' AND '2022-12-31'`. Por lo tanto, en los `PartitionFilters`, verás que Spark ha resuelto esto a un rango y escaneará únicamente las 31 particiones de diciembre de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062306b7-d2b8-435b-99eb-dee5ffcb19d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM devoluciones_silver \n",
    "WHERE id_fecha IN ('2025-02-01', '2024-03-15');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16eae73c-3484-4d33-a58d-2104ef24fd0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "EXPLAIN SELECT * FROM devoluciones_silver \n",
    "WHERE id_fecha IN ('2023-02-01', '2023-03-15');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567c47ab-af7b-48aa-9f77-6a5c704eff64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "EXPLAIN SELECT motivo, COUNT(id_pedido) \n",
    "FROM devoluciones_silver \n",
    "WHERE id_fecha BETWEEN '2024-01-01' AND '2024-01-07'\n",
    "GROUP BY motivo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9598d489-3395-42ab-a7d1-23d30e661df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT motivo, COUNT(id_pedido) \n",
    "FROM devoluciones_silver \n",
    "WHERE id_fecha BETWEEN '2024-01-01' AND '2024-01-07'\n",
    "GROUP BY motivo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d453d6a-ae10-4b7f-891d-6d3d189c7f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "EXPLAIN SELECT * FROM devoluciones_silver \n",
    "WHERE YEAR(id_fecha) = 2024 AND MONTH(id_fecha) = 12;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738c68cb-4b8c-423f-b139-4f6f9872975d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM devoluciones_silver \n",
    "WHERE YEAR(id_fecha) = 2024 AND MONTH(id_fecha) = 12;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4a6b23f-671c-49ec-90c7-c00e5df099dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Conclusión**\n",
    "\n",
    "Hemos verificado empíricamente que nuestra decisión de diseño de particionar la tabla `pedidos_silver` por `id_fecha` fue correcta. El comando `EXPLAIN` nos ha permitido visualizar el \"data skipping\" en acción a través de la sección `PartitionFilters` en múltiples escenarios.\n",
    "\n",
    "Esta técnica es la optimización de rendimiento más importante para tablas de hechos grandes en un Data Warehouse, ya que la mayoría de las consultas analíticas se realizan sobre rangos de tiempo específicos. En el próximo cuaderno, exploraremos la otra técnica de optimización que aplicamos: **Z-Ordering**."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7502565160547261,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "particiones",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
